{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we doing?\n",
    "\n",
    "## Objectives \n",
    "\n",
    "\n",
    "* Build a data pipeline that downloads price data from the internet, stores it locally, transforms it into return data, and stores the feature set.\n",
    "    - Getting the data.\n",
    "    - Schemas and index in dask.\n",
    "\n",
    "* Explore the parquet format.\n",
    "    - Reading and writing parquet files.\n",
    "    - Read datasets that are stored in distributed files.\n",
    "    - Discuss dask vs pandas as a small example of big vs small data.\n",
    "    \n",
    "* Discuss the use of environment variables for settings.\n",
    "* Discuss how to use Jupyter notebooks and source code concurrently. \n",
    "* Logging and using a standard logger.\n",
    "\n",
    "## About the Data\n",
    "\n",
    "+ We will download the prices for a list of stocks.\n",
    "+ The source is Yahoo Finance and we will use the API provided by the library yfinance.\n",
    "\n",
    "\n",
    "## Medallion Architecture\n",
    "\n",
    "+ The architecture that we are thinking about is called Medallion by [DataBricks](https://www.databricks.com/glossary/medallion-architecture). It is an ELT type of thinking, although our data is well-structured.\n",
    "\n",
    "![Medallion Architecture (DataBicks)](./images/02_medallion_architecture.png)\n",
    "\n",
    "+ In our case, we would like to optimize the number of times that we download data from the internet. \n",
    "+ Ultimately, we will build a pipeline manager class that will help us control the process of obtaining and transforming our data.\n",
    "\n",
    "![](./images/02_target_pipeline_manager.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "\n",
    "Download the [Stock Market Dataset from Kaggle](https://www.kaggle.com/datasets/jacksoncrow/stock-market-dataset). Note that you may be required to register for a free account.\n",
    "\n",
    "Extract all files into the directory: `./05_src/data/prices_csv/`\n",
    "\n",
    "Your folder structure should include the following paths:\n",
    "\n",
    "+ `05_src/data/prices_csv/etfs`\n",
    "+ `05_src/data/prices_csv/stocks`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "sys.path.append(os.getenv('SRC_DIR'))\n",
    "\n",
    "from utils.logger import get_logger\n",
    "_logs = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to notice in the code chunk above:\n",
    "\n",
    "+ Libraries are ordered from high-level to low-level libraries from the package manager (pip in this case, but could be conda, poetry, etc.)\n",
    "+ The command `sys.path.append(\"../05_src/)` will add the `../05_src/` directory to the path in the Notebook's kernel. This way, we can use our modules as part of the notebook.\n",
    "+ Local modules are imported at the end. \n",
    "+ The function `get_logger()` is called with `__name__` as recommended by the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to load the historical price data for stocks and ETFs, we could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 19:31:08,498, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PAC.csv\n",
      "2025-10-02 19:31:08,570, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/BOOM.csv\n",
      "2025-10-02 19:31:08,570, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/BOOM.csv\n",
      "2025-10-02 19:31:08,625, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/MNKD.csv\n",
      "2025-10-02 19:31:08,625, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/MNKD.csv\n",
      "2025-10-02 19:31:08,688, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/LGI.csv\n",
      "2025-10-02 19:31:08,688, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/LGI.csv\n",
      "2025-10-02 19:31:08,756, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/J.csv\n",
      "2025-10-02 19:31:08,756, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/J.csv\n",
      "2025-10-02 19:31:08,823, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/MCX.csv\n",
      "2025-10-02 19:31:08,823, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/MCX.csv\n",
      "2025-10-02 19:31:08,929, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/RNWK.csv\n",
      "2025-10-02 19:31:08,929, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/RNWK.csv\n",
      "2025-10-02 19:31:09,001, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ZYNE.csv\n",
      "2025-10-02 19:31:09,001, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ZYNE.csv\n",
      "2025-10-02 19:31:09,062, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/TRVG.csv\n",
      "2025-10-02 19:31:09,062, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/TRVG.csv\n",
      "2025-10-02 19:31:09,157, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/LH.csv\n",
      "2025-10-02 19:31:09,157, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/LH.csv\n",
      "2025-10-02 19:31:09,261, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/IPAR.csv\n",
      "2025-10-02 19:31:09,261, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/IPAR.csv\n",
      "2025-10-02 19:31:09,310, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/NTIP.csv\n",
      "2025-10-02 19:31:09,310, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/NTIP.csv\n",
      "2025-10-02 19:31:09,364, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/UCBI.csv\n",
      "2025-10-02 19:31:09,364, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/UCBI.csv\n",
      "2025-10-02 19:31:09,434, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/NNVC.csv\n",
      "2025-10-02 19:31:09,434, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/NNVC.csv\n",
      "2025-10-02 19:31:09,498, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/FIT.csv\n",
      "2025-10-02 19:31:09,498, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/FIT.csv\n",
      "2025-10-02 19:31:09,619, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/WABC.csv\n",
      "2025-10-02 19:31:09,619, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/WABC.csv\n",
      "2025-10-02 19:31:09,691, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/MCI.csv\n",
      "2025-10-02 19:31:09,691, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/MCI.csv\n",
      "2025-10-02 19:31:09,764, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/RLJ.csv\n",
      "2025-10-02 19:31:09,764, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/RLJ.csv\n",
      "2025-10-02 19:31:09,808, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/MTN.csv\n",
      "2025-10-02 19:31:09,808, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/MTN.csv\n",
      "2025-10-02 19:31:09,852, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PACQU.csv\n",
      "2025-10-02 19:31:09,852, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PACQU.csv\n",
      "2025-10-02 19:31:09,913, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/GSM.csv\n",
      "2025-10-02 19:31:09,913, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/GSM.csv\n",
      "2025-10-02 19:31:09,957, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SND.csv\n",
      "2025-10-02 19:31:09,962, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/EAB.csv\n",
      "2025-10-02 19:31:09,957, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SND.csv\n",
      "2025-10-02 19:31:09,962, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/EAB.csv\n",
      "2025-10-02 19:31:09,998, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/GTHX.csv\n",
      "2025-10-02 19:31:09,998, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/GTHX.csv\n",
      "2025-10-02 19:31:10,041, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SSYS.csv\n",
      "2025-10-02 19:31:10,041, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SSYS.csv\n",
      "2025-10-02 19:31:10,089, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/EFT.csv\n",
      "2025-10-02 19:31:10,100, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/AYX.csv\n",
      "2025-10-02 19:31:10,089, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/EFT.csv\n",
      "2025-10-02 19:31:10,100, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/AYX.csv\n",
      "2025-10-02 19:31:10,166, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/EOG.csv\n",
      "2025-10-02 19:31:10,166, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/EOG.csv\n",
      "2025-10-02 19:31:10,209, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/HBNC.csv\n",
      "2025-10-02 19:31:10,209, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/HBNC.csv\n",
      "2025-10-02 19:31:10,265, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CTLT.csv\n",
      "2025-10-02 19:31:10,272, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/FIVN.csv\n",
      "2025-10-02 19:31:10,265, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CTLT.csv\n",
      "2025-10-02 19:31:10,272, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/FIVN.csv\n",
      "2025-10-02 19:31:10,311, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/KN.csv\n",
      "2025-10-02 19:31:10,311, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/KN.csv\n",
      "2025-10-02 19:31:10,390, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/QTRX.csv\n",
      "2025-10-02 19:31:10,390, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/QTRX.csv\n",
      "2025-10-02 19:31:10,462, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/WEBK.csv\n",
      "2025-10-02 19:31:10,462, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/WEBK.csv\n",
      "2025-10-02 19:31:10,492, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CQP.csv\n",
      "2025-10-02 19:31:10,492, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CQP.csv\n",
      "2025-10-02 19:31:10,608, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/GSH.csv\n",
      "2025-10-02 19:31:10,608, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/GSH.csv\n",
      "2025-10-02 19:31:10,652, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ANDE.csv\n",
      "2025-10-02 19:31:10,652, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ANDE.csv\n",
      "2025-10-02 19:31:10,712, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/LMRKO.csv\n",
      "2025-10-02 19:31:10,712, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/LMRKO.csv\n",
      "2025-10-02 19:31:10,792, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/TG.csv\n",
      "2025-10-02 19:31:10,792, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/TG.csv\n",
      "2025-10-02 19:31:10,843, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/QDEL.csv\n",
      "2025-10-02 19:31:10,843, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/QDEL.csv\n",
      "2025-10-02 19:31:10,906, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ARL.csv\n",
      "2025-10-02 19:31:10,906, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ARL.csv\n",
      "2025-10-02 19:31:10,951, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/DZSI.csv\n",
      "2025-10-02 19:31:10,951, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/DZSI.csv\n",
      "2025-10-02 19:31:11,006, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ETW.csv\n",
      "2025-10-02 19:31:11,006, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ETW.csv\n",
      "2025-10-02 19:31:11,067, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/UBP.csv\n",
      "2025-10-02 19:31:11,067, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/UBP.csv\n",
      "2025-10-02 19:31:11,179, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CHWY.csv\n",
      "2025-10-02 19:31:11,179, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CHWY.csv\n",
      "2025-10-02 19:31:11,211, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/TX.csv\n",
      "2025-10-02 19:31:11,211, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/TX.csv\n",
      "2025-10-02 19:31:11,252, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ENZ.csv\n",
      "2025-10-02 19:31:11,252, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ENZ.csv\n",
      "2025-10-02 19:31:11,322, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ABMD.csv\n",
      "2025-10-02 19:31:11,322, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ABMD.csv\n",
      "2025-10-02 19:31:11,393, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/OFED.csv\n",
      "2025-10-02 19:31:11,393, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/OFED.csv\n",
      "2025-10-02 19:31:11,433, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/RBNC.csv\n",
      "2025-10-02 19:31:11,433, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/RBNC.csv\n",
      "2025-10-02 19:31:11,501, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/MAG.csv\n",
      "2025-10-02 19:31:11,501, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/MAG.csv\n",
      "2025-10-02 19:31:11,565, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/INSUU.csv\n",
      "2025-10-02 19:31:11,565, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/INSUU.csv\n",
      "2025-10-02 19:31:11,612, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/IDYA.csv\n",
      "2025-10-02 19:31:11,612, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/IDYA.csv\n",
      "2025-10-02 19:31:11,661, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/IBIO.csv\n",
      "2025-10-02 19:31:11,661, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/IBIO.csv\n",
      "2025-10-02 19:31:11,710, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/TRIB.csv\n",
      "2025-10-02 19:31:11,710, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/TRIB.csv\n",
      "2025-10-02 19:31:11,757, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CXDC.csv\n",
      "2025-10-02 19:31:11,757, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CXDC.csv\n",
      "2025-10-02 19:31:11,789, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/HIW.csv\n",
      "2025-10-02 19:31:11,789, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/HIW.csv\n",
      "2025-10-02 19:31:11,875, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CRAI.csv\n",
      "2025-10-02 19:31:11,886, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/WDR.csv\n",
      "2025-10-02 19:31:11,875, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CRAI.csv\n",
      "2025-10-02 19:31:11,886, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/WDR.csv\n",
      "2025-10-02 19:31:11,955, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/GHL.csv\n",
      "2025-10-02 19:31:11,955, 4121356139.py, 10, INFO, Reading file: ../../05_src/data/prices_csv/stocks/GHL.csv\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "stock_files = glob(os.path.join(os.getenv('SRC_DIR'), \"data/prices_csv/stocks/*.csv\"))\n",
    "\n",
    "random.seed(42)\n",
    "stock_files = random.sample(stock_files, 60)\n",
    "\n",
    "dt_list = []\n",
    "for s_file in stock_files:\n",
    "    _logs.info(f\"Reading file: {s_file}\")\n",
    "    dt = pd.read_csv(s_file).assign(\n",
    "        source = os.path.basename(s_file),\n",
    "        ticker = os.path.basename(s_file).replace('.csv', ''),\n",
    "        Date = lambda x: pd.to_datetime(x['Date'])\n",
    "    )\n",
    "    dt_list.append(dt)\n",
    "stock_prices = pd.concat(dt_list, axis = 0, ignore_index = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the structure of the `stock_prices` data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 256050 entries, 0 to 256049\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count   Dtype         \n",
      "---  ------     --------------   -----         \n",
      " 0   Date       256050 non-null  datetime64[ns]\n",
      " 1   Open       256048 non-null  float64       \n",
      " 2   High       256048 non-null  float64       \n",
      " 3   Low        256048 non-null  float64       \n",
      " 4   Close      256048 non-null  float64       \n",
      " 5   Adj Close  256048 non-null  float64       \n",
      " 6   Volume     256048 non-null  float64       \n",
      " 7   source     256050 non-null  object        \n",
      " 8   ticker     256050 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(6), object(2)\n",
      "memory usage: 17.6+ MB\n"
     ]
    }
   ],
   "source": [
    "stock_prices.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subset our ticker data set using standard indexing techniques. A good reference for this type of data manipulation is Panda's [Documentation](https://pandas.pydata.org/docs/user_guide/indexing.html#indexing-and-selecting-data) and [Cookbook](https://pandas.pydata.org/docs/user_guide/cookbook.html#cookbook-selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the subset data frame, select one column and convert to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAC',\n",
       " 'BOOM',\n",
       " 'MNKD',\n",
       " 'LGI',\n",
       " 'J',\n",
       " 'MCX',\n",
       " 'RNWK',\n",
       " 'ZYNE',\n",
       " 'TRVG',\n",
       " 'LH',\n",
       " 'IPAR',\n",
       " 'NTIP',\n",
       " 'UCBI',\n",
       " 'NNVC',\n",
       " 'FIT',\n",
       " 'WABC',\n",
       " 'MCI',\n",
       " 'RLJ',\n",
       " 'MTN',\n",
       " 'PACQU',\n",
       " 'GSM',\n",
       " 'SND',\n",
       " 'EAB',\n",
       " 'GTHX',\n",
       " 'SSYS',\n",
       " 'EFT',\n",
       " 'AYX',\n",
       " 'EOG',\n",
       " 'HBNC',\n",
       " 'CTLT',\n",
       " 'FIVN',\n",
       " 'KN',\n",
       " 'QTRX',\n",
       " 'WEBK',\n",
       " 'CQP',\n",
       " 'GSH',\n",
       " 'ANDE',\n",
       " 'LMRKO',\n",
       " 'TG',\n",
       " 'QDEL',\n",
       " 'ARL',\n",
       " 'DZSI',\n",
       " 'ETW',\n",
       " 'UBP',\n",
       " 'CHWY',\n",
       " 'TX',\n",
       " 'ENZ',\n",
       " 'ABMD',\n",
       " 'OFED',\n",
       " 'RBNC',\n",
       " 'MAG',\n",
       " 'INSUU',\n",
       " 'IDYA',\n",
       " 'IBIO',\n",
       " 'TRIB',\n",
       " 'CXDC',\n",
       " 'HIW',\n",
       " 'CRAI',\n",
       " 'WDR',\n",
       " 'GHL']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_tickers = stock_prices['ticker'].unique().tolist()\n",
    "select_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing Data in CSV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ We have some data. How do we store it?\n",
    "+ We can compare two options, CSV and Parqruet, by measuring their performance:\n",
    "\n",
    "    - Time to save.\n",
    "    - Space required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_size(path='.'):\n",
    "    '''Returns the total size of files contained in path.'''\n",
    "    total = 0\n",
    "    with os.scandir(path) as it:\n",
    "        for entry in it:\n",
    "            if entry.is_file():\n",
    "                total += entry.stat().st_size\n",
    "            elif entry.is_dir():\n",
    "                total += get_dir_size(entry.path)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = os.getenv(\"TEMP_DATA\")\n",
    "csv_dir = os.path.join(temp, \"csv\")\n",
    "shutil.rmtree(csv_dir, ignore_errors=True)\n",
    "stock_csv = os.path.join(csv_dir, \"stock_px.csv\")\n",
    "os.makedirs(csv_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 19:31:14,537, 473192941.py, 5, INFO, Writing data ((256050, 9)) to csv took 2.4330239295959473 seconds.\n",
      "2025-10-02 19:31:14,538, 473192941.py, 6, INFO, CSV file size 28.123426 MB\n",
      "2025-10-02 19:31:14,538, 473192941.py, 6, INFO, CSV file size 28.123426 MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "stock_prices.to_csv(stock_csv, index = False)\n",
    "end = time.time()\n",
    "\n",
    "_logs.info(f'Writing data ({stock_prices.shape}) to csv took {end - start} seconds.')\n",
    "_logs.info(f'CSV file size { os.path.getsize(stock_csv)*1e-6 } MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to Parquet\n",
    "\n",
    "### Dask \n",
    "\n",
    "We can work with with large data sets and parquet files. In fact, recent versions of pandas support pyarrow data types and future versions will require a pyarrow backend. The pyarrow library is an interface between Python and the Appache Arrow project. The [parquet data format](https://parquet.apache.org/) and [Arrow](https://arrow.apache.org/docs/python/parquet.html) are projects of the Apache Foundation.\n",
    "\n",
    "However, Dask is much more than an interface to Arrow: Dask provides parallel and distributed computing on pandas-like dataframes. It is also relatively easy to use, bridging a gap between pandas and Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "parquet_dir = os.path.join(temp, \"parquet\")\n",
    "shutil.rmtree(parquet_dir, ignore_errors=True)\n",
    "os.makedirs(parquet_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.3.2\n",
      "dask: 2025.7.0\n",
      "pyarrow: 19.0.0\n"
     ]
    }
   ],
   "source": [
    "# Check if required packages are installed and print their versions\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import dask\n",
    "    import dask.dataframe as dd\n",
    "    import pyarrow\n",
    "    print('pandas:', pd.__version__)\n",
    "    print('dask:', dask.__version__)\n",
    "    print('pyarrow:', pyarrow.__version__)\n",
    "except ImportError as e:\n",
    "    print('ImportError:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 19:31:23,534, 817812245.py, 7, INFO, Writing dd ((256050, 9)) to parquet took 1.8489189147949219 seconds.\n",
      "2025-10-02 19:31:23,535, 817812245.py, 8, INFO, Parquet file size 9.870593999999999 MB\n",
      "2025-10-02 19:31:23,535, 817812245.py, 8, INFO, Parquet file size 9.870593999999999 MB\n"
     ]
    }
   ],
   "source": [
    "px_dd = dd.from_pandas(stock_prices, npartitions = len(select_tickers))\n",
    "\n",
    "start = time.time()\n",
    "px_dd.to_parquet(parquet_dir, engine = \"pyarrow\")\n",
    "end = time.time()\n",
    "\n",
    "_logs.info(f'Writing dd ({stock_prices.shape}) to parquet took {end - start} seconds.')\n",
    "_logs.info(f'Parquet file size { get_dir_size(parquet_dir)*1e-6 } MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet files and Dask Dataframes\n",
    "\n",
    "+ Parquet files are immutable: once written, they cannot be modified.\n",
    "+ Dask DataFrames are a useful implementation to manipulate data stored in parquets.\n",
    "+ Parquet and Dask are not the same: parquet is a file format that can be accessed by many applications and programming languages (Python, R, PowerBI, etc.), while Dask is a package in Python to work with large datasets using distributed computation.\n",
    "+ **Dask is not for everything** (see [Dask DataFrames Best Practices](https://docs.dask.org/en/stable/dataframe-best-practices.html)). \n",
    "\n",
    "    - Consider cases suchas small to large joins, where the small dataframe fits in memory, but the large one does not. \n",
    "    - If possible, use pandas: reduce, then use pandas.\n",
    "    - Pandas performance tips apply to Dask.\n",
    "    - Use the index: it is beneficial to have a well-defined index in Dask DataFrames, as it may speed up searching (filtering) the data. A one-dimensional index is allowed.\n",
    "    - Avoid (or minimize) full-data shuffling: indexing is an expensive operations. \n",
    "    - Some joins are more expensive than others. \n",
    "\n",
    "        * Not expensive:\n",
    "\n",
    "            - Join a Dask DataFrame with a pandas DataFrame.\n",
    "            - Join a Dask DataFrame with another Dask DataFrame of a single partition.\n",
    "            - Join Dask DataFrames along their indexes.\n",
    "\n",
    "        * Expensive:\n",
    "\n",
    "            - Join Dask DataFrames along columns that are not their index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we store prices?\n",
    "\n",
    "+ We can store our data as a single blob. This can be difficult to maintain, especially because parquet files are immutable.\n",
    "+ Strategy: organize data files by ticker and date. Update only latest month.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLean up before start\n",
    "PRICE_DATA = os.getenv(\"PRICE_DATA\")\n",
    "import shutil\n",
    "if os.path.exists(PRICE_DATA):\n",
    "    shutil.rmtree(PRICE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'source',\n",
       "       'ticker'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_prices.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in stock_prices['ticker'].unique():\n",
    "    ticker_dt = stock_prices[stock_prices['ticker'] == ticker]\n",
    "    ticker_dt = ticker_dt.assign(Year = ticker_dt.Date.dt.year)\n",
    "    for yr in ticker_dt['Year'].unique():\n",
    "        yr_dd = dd.from_pandas(ticker_dt[ticker_dt['Year'] == yr],2)\n",
    "        yr_path = os.path.join(PRICE_DATA, ticker, f\"{ticker}_{yr}\")\n",
    "        os.makedirs(os.path.dirname(yr_path), exist_ok=True)\n",
    "        yr_dd.to_parquet(yr_path, engine = \"pyarrow\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would we want to store data this way?\n",
    "\n",
    "+ Easier to maintain. We do not update old data, only recent data.\n",
    "+ We can also access all files as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Transform and Save "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load\n",
    "\n",
    "+ Parquet files can be read individually or as a collection.\n",
    "+ `dd.read_parquet()` can take a list (collection) of files as input.\n",
    "+ Use `glob` to get the collection of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "parquet_files = glob(os.path.join(PRICE_DATA, \"**/*.parquet\"), recursive = True)\n",
    "dd_px = dd.read_parquet(parquet_files).set_index(\"ticker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform\n",
    "\n",
    "+ This transformation step will create a *Features* data set. In our case, features will be stock returns (we obtained prices).\n",
    "+ Dask dataframes work like pandas dataframes: in particular, we can perform groupby and apply operations.\n",
    "+ Notice the use of [an anonymous (lambda) function](https://realpython.com/python-lambda/) in the apply statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3093/102405636.py:1: UserWarning: `meta` is not specified, inferred from partial data.\n",
      "Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "\n",
      "  dd_shift = dd_px.groupby('ticker', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "dd_shift = dd_px.groupby('ticker', group_keys=False).apply(\n",
    "    lambda x: x.assign(Close_lag_1 = x['Close'].shift(1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_rets = dd_shift.assign(\n",
    "    Returns = lambda x: x['Close']/x['Close_lag_1'] - 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy Exection\n",
    "\n",
    "What does `dd_rets` contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>source</th>\n",
       "      <th>Year</th>\n",
       "      <th>Close_lag_1</th>\n",
       "      <th>Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=60</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABMD</th>\n",
       "      <td>datetime64[ns]</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>string</td>\n",
       "      <td>int32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ANDE</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYNE</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYNE</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: assign, 9 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                          Date     Open     High      Low    Close Adj Close   Volume  source   Year Close_lag_1  Returns\n",
       "npartitions=60                                                                                                           \n",
       "ABMD            datetime64[ns]  float64  float64  float64  float64   float64  float64  string  int32     float64  float64\n",
       "ANDE                       ...      ...      ...      ...      ...       ...      ...     ...    ...         ...      ...\n",
       "...                        ...      ...      ...      ...      ...       ...      ...     ...    ...         ...      ...\n",
       "ZYNE                       ...      ...      ...      ...      ...       ...      ...     ...    ...         ...      ...\n",
       "ZYNE                       ...      ...      ...      ...      ...       ...      ...     ...    ...         ...      ...\n",
       "Dask Name: assign, 9 expressions\n",
       "Expr=Assign(frame=Assign(frame=GroupByApply(frame=SetIndex(frame=ReadParquetFSSpec(3946417), _other='ticker', options={}), observed=False, group_keys=False, func=<function <lambda> at 0x7839c3607560>, meta=<no_default>, args=(), kwargs={})))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_rets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Dask is a lazy execution framework: commands will not execute until they are required. \n",
    "+ To trigger an execution in dask use `.compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>source</th>\n",
       "      <th>Year</th>\n",
       "      <th>Close_lag_1</th>\n",
       "      <th>Returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ABMD</th>\n",
       "      <td>2005-01-03</td>\n",
       "      <td>15.87</td>\n",
       "      <td>15.970</td>\n",
       "      <td>14.970</td>\n",
       "      <td>15.01</td>\n",
       "      <td>15.01</td>\n",
       "      <td>213500.0</td>\n",
       "      <td>ABMD.csv</td>\n",
       "      <td>2005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABMD</th>\n",
       "      <td>2005-01-04</td>\n",
       "      <td>15.18</td>\n",
       "      <td>15.200</td>\n",
       "      <td>14.760</td>\n",
       "      <td>14.98</td>\n",
       "      <td>14.98</td>\n",
       "      <td>140300.0</td>\n",
       "      <td>ABMD.csv</td>\n",
       "      <td>2005</td>\n",
       "      <td>15.01</td>\n",
       "      <td>-0.001999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABMD</th>\n",
       "      <td>2005-01-05</td>\n",
       "      <td>14.75</td>\n",
       "      <td>15.100</td>\n",
       "      <td>14.210</td>\n",
       "      <td>14.69</td>\n",
       "      <td>14.69</td>\n",
       "      <td>160600.0</td>\n",
       "      <td>ABMD.csv</td>\n",
       "      <td>2005</td>\n",
       "      <td>14.98</td>\n",
       "      <td>-0.019359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABMD</th>\n",
       "      <td>2005-01-06</td>\n",
       "      <td>14.49</td>\n",
       "      <td>14.790</td>\n",
       "      <td>14.270</td>\n",
       "      <td>14.52</td>\n",
       "      <td>14.52</td>\n",
       "      <td>194900.0</td>\n",
       "      <td>ABMD.csv</td>\n",
       "      <td>2005</td>\n",
       "      <td>14.69</td>\n",
       "      <td>-0.011572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABMD</th>\n",
       "      <td>2005-01-07</td>\n",
       "      <td>14.56</td>\n",
       "      <td>14.560</td>\n",
       "      <td>13.740</td>\n",
       "      <td>14.29</td>\n",
       "      <td>14.29</td>\n",
       "      <td>97800.0</td>\n",
       "      <td>ABMD.csv</td>\n",
       "      <td>2005</td>\n",
       "      <td>14.52</td>\n",
       "      <td>-0.015840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYNE</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>3.86</td>\n",
       "      <td>4.040</td>\n",
       "      <td>3.800</td>\n",
       "      <td>3.94</td>\n",
       "      <td>3.94</td>\n",
       "      <td>400700.0</td>\n",
       "      <td>ZYNE.csv</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.83</td>\n",
       "      <td>0.028721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYNE</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>3.84</td>\n",
       "      <td>4.000</td>\n",
       "      <td>3.630</td>\n",
       "      <td>3.81</td>\n",
       "      <td>3.81</td>\n",
       "      <td>383100.0</td>\n",
       "      <td>ZYNE.csv</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.94</td>\n",
       "      <td>-0.032995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYNE</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>3.83</td>\n",
       "      <td>3.892</td>\n",
       "      <td>3.510</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>321100.0</td>\n",
       "      <td>ZYNE.csv</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.81</td>\n",
       "      <td>-0.036745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYNE</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>3.64</td>\n",
       "      <td>3.840</td>\n",
       "      <td>3.568</td>\n",
       "      <td>3.83</td>\n",
       "      <td>3.83</td>\n",
       "      <td>391200.0</td>\n",
       "      <td>ZYNE.csv</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.043597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZYNE</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>3.73</td>\n",
       "      <td>3.790</td>\n",
       "      <td>3.377</td>\n",
       "      <td>3.41</td>\n",
       "      <td>3.41</td>\n",
       "      <td>324800.0</td>\n",
       "      <td>ZYNE.csv</td>\n",
       "      <td>2020</td>\n",
       "      <td>3.83</td>\n",
       "      <td>-0.109661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256050 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date   Open    High     Low  Close  Adj Close    Volume  \\\n",
       "ticker                                                                 \n",
       "ABMD   2005-01-03  15.87  15.970  14.970  15.01      15.01  213500.0   \n",
       "ABMD   2005-01-04  15.18  15.200  14.760  14.98      14.98  140300.0   \n",
       "ABMD   2005-01-05  14.75  15.100  14.210  14.69      14.69  160600.0   \n",
       "ABMD   2005-01-06  14.49  14.790  14.270  14.52      14.52  194900.0   \n",
       "ABMD   2005-01-07  14.56  14.560  13.740  14.29      14.29   97800.0   \n",
       "...           ...    ...     ...     ...    ...        ...       ...   \n",
       "ZYNE   2020-03-26   3.86   4.040   3.800   3.94       3.94  400700.0   \n",
       "ZYNE   2020-03-27   3.84   4.000   3.630   3.81       3.81  383100.0   \n",
       "ZYNE   2020-03-30   3.83   3.892   3.510   3.67       3.67  321100.0   \n",
       "ZYNE   2020-03-31   3.64   3.840   3.568   3.83       3.83  391200.0   \n",
       "ZYNE   2020-04-01   3.73   3.790   3.377   3.41       3.41  324800.0   \n",
       "\n",
       "          source  Year  Close_lag_1   Returns  \n",
       "ticker                                         \n",
       "ABMD    ABMD.csv  2005          NaN       NaN  \n",
       "ABMD    ABMD.csv  2005        15.01 -0.001999  \n",
       "ABMD    ABMD.csv  2005        14.98 -0.019359  \n",
       "ABMD    ABMD.csv  2005        14.69 -0.011572  \n",
       "ABMD    ABMD.csv  2005        14.52 -0.015840  \n",
       "...          ...   ...          ...       ...  \n",
       "ZYNE    ZYNE.csv  2020         3.83  0.028721  \n",
       "ZYNE    ZYNE.csv  2020         3.94 -0.032995  \n",
       "ZYNE    ZYNE.csv  2020         3.81 -0.036745  \n",
       "ZYNE    ZYNE.csv  2020         3.67  0.043597  \n",
       "ZYNE    ZYNE.csv  2020         3.83 -0.109661  \n",
       "\n",
       "[256050 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_rets.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save\n",
    "\n",
    "+ Apply transformations to calculate daily returns\n",
    "+ Store the enriched data, the silver dataset, in a new directory.\n",
    "+ Should we keep the same namespace? All columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLean up before save\n",
    "FEATURES_DATA = os.getenv(\"FEATURES_DATA\")\n",
    "if os.path.exists(FEATURES_DATA):\n",
    "    shutil.rmtree(FEATURES_DATA)\n",
    "dd_rets.to_parquet(FEATURES_DATA, overwrite = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: from Jupyter to Command Line\n",
    "\n",
    "+ We have drafted our code in a Jupyter Notebook. \n",
    "+ Finalized code should be written in Python modules.\n",
    "\n",
    "## Object Oriented vs Functional Programming\n",
    "\n",
    "+ We can use classes to keep parameters and functions together.\n",
    "+ We *could* use Object Oriented Programming, but parallelization of data manipulation and modelling tasks benefit from *Functional Programming*.\n",
    "+ An Idea: \n",
    "\n",
    "    - [Data Oriented Programming](https://blog.klipse.tech/dop/2022/06/22/principles-of-dop.html).\n",
    "    - Use the class to bundle together parameters and functions.\n",
    "    - Use stateless operations and treat all data objects as immutable (we do not modify them, we overwrite them).\n",
    "    - Take advantage of [`@staticmethod`](https://realpython.com/instance-class-and-static-methods-demystified/).\n",
    "\n",
    "The code is in `./05_src/stock_prices/data_manager.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original design was:\n",
    "\n",
    "![](./images/02_target_pipeline_manager.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stock_prices.data_manager import DataManager\n",
    "dm = DataManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download all prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 19:32:04,746, data_manager.py, 53, INFO, Processing sample of tickers\n",
      "2025-10-02 19:32:04,749, data_manager.py, 64, INFO, Getting file list from ../../05_src/data/prices_csv/\n",
      "2025-10-02 19:32:04,749, data_manager.py, 64, INFO, Getting file list from ../../05_src/data/prices_csv/\n",
      "2025-10-02 19:32:04,776, data_manager.py, 66, INFO, Found 8050 files in ../../05_src/data/prices/\n",
      "2025-10-02 19:32:04,777, data_manager.py, 74, INFO, Selecting sample of files\n",
      "2025-10-02 19:32:04,778, data_manager.py, 80, INFO, Selected 30 files\n",
      "2025-10-02 19:32:04,778, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/SHIP.csv\n",
      "2025-10-02 19:32:04,779, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SHIP.csv\n",
      "2025-10-02 19:32:04,776, data_manager.py, 66, INFO, Found 8050 files in ../../05_src/data/prices/\n",
      "2025-10-02 19:32:04,777, data_manager.py, 74, INFO, Selecting sample of files\n",
      "2025-10-02 19:32:04,778, data_manager.py, 80, INFO, Selected 30 files\n",
      "2025-10-02 19:32:04,778, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/SHIP.csv\n",
      "2025-10-02 19:32:04,779, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SHIP.csv\n",
      "2025-10-02 19:32:04,827, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:04,827, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,063, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/NIO.csv\n",
      "2025-10-02 19:32:05,064, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/NIO.csv\n",
      "2025-10-02 19:32:05,068, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,063, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/NIO.csv\n",
      "2025-10-02 19:32:05,064, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/NIO.csv\n",
      "2025-10-02 19:32:05,068, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,117, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/LSAF.csv\n",
      "2025-10-02 19:32:05,118, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/LSAF.csv\n",
      "2025-10-02 19:32:05,124, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,117, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/LSAF.csv\n",
      "2025-10-02 19:32:05,118, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/LSAF.csv\n",
      "2025-10-02 19:32:05,124, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,206, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/XLRN.csv\n",
      "2025-10-02 19:32:05,207, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/XLRN.csv\n",
      "2025-10-02 19:32:05,206, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/XLRN.csv\n",
      "2025-10-02 19:32:05,207, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/XLRN.csv\n",
      "2025-10-02 19:32:05,244, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,244, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,469, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/PNNT.csv\n",
      "2025-10-02 19:32:05,470, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PNNT.csv\n",
      "2025-10-02 19:32:05,469, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/PNNT.csv\n",
      "2025-10-02 19:32:05,470, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PNNT.csv\n",
      "2025-10-02 19:32:05,509, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,509, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,738, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/PRTA.csv\n",
      "2025-10-02 19:32:05,739, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PRTA.csv\n",
      "2025-10-02 19:32:05,738, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/PRTA.csv\n",
      "2025-10-02 19:32:05,739, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PRTA.csv\n",
      "2025-10-02 19:32:05,766, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,766, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:05,971, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/ALNA.csv\n",
      "2025-10-02 19:32:05,972, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ALNA.csv\n",
      "2025-10-02 19:32:05,971, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/ALNA.csv\n",
      "2025-10-02 19:32:05,972, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ALNA.csv\n",
      "2025-10-02 19:32:06,013, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:06,013, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:06,090, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/EEH.csv\n",
      "2025-10-02 19:32:06,090, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/EEH.csv\n",
      "2025-10-02 19:32:06,090, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/EEH.csv\n",
      "2025-10-02 19:32:06,090, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/EEH.csv\n",
      "2025-10-02 19:32:06,124, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:06,124, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:06,392, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/PFHD.csv\n",
      "2025-10-02 19:32:06,392, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PFHD.csv\n",
      "2025-10-02 19:32:06,392, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/PFHD.csv\n",
      "2025-10-02 19:32:06,392, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PFHD.csv\n",
      "2025-10-02 19:32:06,424, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:06,424, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:06,492, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/FDN.csv\n",
      "2025-10-02 19:32:06,492, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/FDN.csv\n",
      "2025-10-02 19:32:06,492, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/FDN.csv\n",
      "2025-10-02 19:32:06,492, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/FDN.csv\n",
      "2025-10-02 19:32:06,597, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:06,597, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:06,903, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/PZZA.csv\n",
      "2025-10-02 19:32:06,903, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PZZA.csv\n",
      "2025-10-02 19:32:06,903, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/PZZA.csv\n",
      "2025-10-02 19:32:06,903, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PZZA.csv\n",
      "2025-10-02 19:32:06,950, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:06,950, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:07,430, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/PACK.csv\n",
      "2025-10-02 19:32:07,430, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PACK.csv\n",
      "2025-10-02 19:32:07,430, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/PACK.csv\n",
      "2025-10-02 19:32:07,430, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/PACK.csv\n",
      "2025-10-02 19:32:07,481, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:07,481, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:07,531, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/LPTX.csv\n",
      "2025-10-02 19:32:07,532, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/LPTX.csv\n",
      "2025-10-02 19:32:07,531, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/LPTX.csv\n",
      "2025-10-02 19:32:07,532, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/LPTX.csv\n",
      "2025-10-02 19:32:07,569, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:07,569, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:07,637, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/FPAY.csv\n",
      "2025-10-02 19:32:07,637, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/FPAY.csv\n",
      "2025-10-02 19:32:07,637, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/FPAY.csv\n",
      "2025-10-02 19:32:07,637, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/FPAY.csv\n",
      "2025-10-02 19:32:07,679, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:07,679, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:07,872, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/TMFC.csv\n",
      "2025-10-02 19:32:07,872, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/TMFC.csv\n",
      "2025-10-02 19:32:07,872, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/TMFC.csv\n",
      "2025-10-02 19:32:07,872, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/TMFC.csv\n",
      "2025-10-02 19:32:07,917, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:07,917, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:07,971, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/ACC.csv\n",
      "2025-10-02 19:32:07,973, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ACC.csv\n",
      "2025-10-02 19:32:07,971, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/ACC.csv\n",
      "2025-10-02 19:32:07,973, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ACC.csv\n",
      "2025-10-02 19:32:08,025, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:08,025, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:08,307, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/STG.csv\n",
      "2025-10-02 19:32:08,307, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/STG.csv\n",
      "2025-10-02 19:32:08,307, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/STG.csv\n",
      "2025-10-02 19:32:08,307, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/STG.csv\n",
      "2025-10-02 19:32:08,354, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:08,354, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:08,641, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/VALT.csv\n",
      "2025-10-02 19:32:08,641, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/VALT.csv\n",
      "2025-10-02 19:32:08,641, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/VALT.csv\n",
      "2025-10-02 19:32:08,641, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/VALT.csv\n",
      "2025-10-02 19:32:08,692, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:08,692, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:08,728, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/CCBG.csv\n",
      "2025-10-02 19:32:08,728, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CCBG.csv\n",
      "2025-10-02 19:32:08,728, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/CCBG.csv\n",
      "2025-10-02 19:32:08,728, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/CCBG.csv\n",
      "2025-10-02 19:32:08,776, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:08,776, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:09,305, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/SPWR.csv\n",
      "2025-10-02 19:32:09,306, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SPWR.csv\n",
      "2025-10-02 19:32:09,305, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/SPWR.csv\n",
      "2025-10-02 19:32:09,306, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SPWR.csv\n",
      "2025-10-02 19:32:09,377, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:09,377, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:09,648, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/SHV.csv\n",
      "2025-10-02 19:32:09,649, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/SHV.csv\n",
      "2025-10-02 19:32:09,648, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/SHV.csv\n",
      "2025-10-02 19:32:09,649, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/SHV.csv\n",
      "2025-10-02 19:32:09,709, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:09,709, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:10,000, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/LRGF.csv\n",
      "2025-10-02 19:32:10,000, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/LRGF.csv\n",
      "2025-10-02 19:32:10,000, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/LRGF.csv\n",
      "2025-10-02 19:32:10,000, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/LRGF.csv\n",
      "2025-10-02 19:32:10,047, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:10,047, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:10,190, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/GNL.csv\n",
      "2025-10-02 19:32:10,196, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/GNL.csv\n",
      "2025-10-02 19:32:10,190, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/GNL.csv\n",
      "2025-10-02 19:32:10,196, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/GNL.csv\n",
      "2025-10-02 19:32:10,238, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:10,238, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:10,526, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/SRCL.csv\n",
      "2025-10-02 19:32:10,526, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SRCL.csv\n",
      "2025-10-02 19:32:10,526, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/SRCL.csv\n",
      "2025-10-02 19:32:10,526, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SRCL.csv\n",
      "2025-10-02 19:32:10,537, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:10,537, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:10,952, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/PSCC.csv\n",
      "2025-10-02 19:32:10,952, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/PSCC.csv\n",
      "2025-10-02 19:32:10,952, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/PSCC.csv\n",
      "2025-10-02 19:32:10,952, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/PSCC.csv\n",
      "2025-10-02 19:32:10,995, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:10,995, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:11,187, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/SMFG.csv\n",
      "2025-10-02 19:32:11,188, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SMFG.csv\n",
      "2025-10-02 19:32:11,195, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:11,187, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/SMFG.csv\n",
      "2025-10-02 19:32:11,188, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/SMFG.csv\n",
      "2025-10-02 19:32:11,195, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:11,553, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/FTXG.csv\n",
      "2025-10-02 19:32:11,555, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/FTXG.csv\n",
      "2025-10-02 19:32:11,553, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/FTXG.csv\n",
      "2025-10-02 19:32:11,555, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/FTXG.csv\n",
      "2025-10-02 19:32:11,607, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:11,607, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:11,718, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/JMBS.csv\n",
      "2025-10-02 19:32:11,719, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/JMBS.csv\n",
      "2025-10-02 19:32:11,718, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/etfs/JMBS.csv\n",
      "2025-10-02 19:32:11,719, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/etfs/JMBS.csv\n",
      "2025-10-02 19:32:11,750, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:11,750, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:11,799, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/ADS.csv\n",
      "2025-10-02 19:32:11,800, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ADS.csv\n",
      "2025-10-02 19:32:11,799, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/ADS.csv\n",
      "2025-10-02 19:32:11,800, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/ADS.csv\n",
      "2025-10-02 19:32:11,838, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:11,838, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:12,192, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/APLS.csv\n",
      "2025-10-02 19:32:12,193, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/APLS.csv\n",
      "2025-10-02 19:32:12,192, data_manager.py, 90, INFO, Processing file ../../05_src/data/prices_csv/stocks/APLS.csv\n",
      "2025-10-02 19:32:12,193, data_manager.py, 118, INFO, Reading file: ../../05_src/data/prices_csv/stocks/APLS.csv\n",
      "2025-10-02 19:32:12,239, data_manager.py, 101, INFO, Saving data by year\n",
      "2025-10-02 19:32:12,239, data_manager.py, 101, INFO, Saving data by year\n"
     ]
    }
   ],
   "source": [
    "dm.process_sample_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, add features to the data set and save to a *feature store*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-02 19:32:12,314, data_manager.py, 131, INFO, Creating features data.\n",
      "2025-10-02 19:32:12,315, data_manager.py, 141, INFO, Loading price data from ../../05_src/data/prices/\n",
      "2025-10-02 19:32:12,315, data_manager.py, 141, INFO, Loading price data from ../../05_src/data/prices/\n",
      "2025-10-02 19:32:12,515, data_manager.py, 150, INFO, Creating features\n",
      "/workspaces/production/01_materials/labs/../../05_src/stock_prices/data_manager.py:154: UserWarning: `meta` is not specified, inferred from partial data.\n",
      "Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "\n",
      "  .apply(\n",
      "2025-10-02 19:32:12,523, data_manager.py, 175, INFO, Saving features to ../../05_src/data/features/stock_features\n",
      "2025-10-02 19:32:12,515, data_manager.py, 150, INFO, Creating features\n",
      "/workspaces/production/01_materials/labs/../../05_src/stock_prices/data_manager.py:154: UserWarning: `meta` is not specified, inferred from partial data.\n",
      "Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "\n",
      "  .apply(\n",
      "2025-10-02 19:32:12,523, data_manager.py, 175, INFO, Saving features to ../../05_src/data/features/stock_features\n"
     ]
    }
   ],
   "source": [
    "dm.featurize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
